<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/ico" href="/favicon.ico"><link rel="stylesheet" href="https://cdn.vidstack.io/player.css"><meta name="generator" content="Astro v5.2.5"><title>Laurent Kempé - Run Phi-3 SLM on your machine with C# Semantic Kernel and Ollama</title><link rel="stylesheet" href="/_astro/_slug_.DKcIe2eh.css">
<style>a[data-astro-cid-5grsw2hi]{color:#00539f}.tags[data-astro-cid-5grsw2hi]{display:flex;flex-wrap:wrap}.tag[data-astro-cid-5grsw2hi]{margin:.25em;border:dotted 1px #a1a1a1;border-radius:.5em;padding:.5em 1em;font-size:1.15em;background-color:#f8fcfd}
</style></head> <body> <header class="py-6 border-b container mx-auto px-4 sm:px-6 lg:px-8 max-w-7xl"> <nav class="container mx-auto px-4 flex flex-col sm:flex-row items-center justify-between"> <div class="text-2xl font-bold mb-4 sm:mb-0"> <a href="/">Laurent Kempé</a> </div> <div class="flex gap-6 text-gray-600"> <a href="/" class="hover:text-gray-900 ">Home</a> <a href="/blog/1" class="hover:text-gray-900 ">Blog</a> <a href="/speaking" class="hover:text-gray-900 ">Speaking</a> <a href="/about-laurent-kempe" class="hover:text-gray-900 ">About</a> </div> </nav> </header> <div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-7xl">  <p data-astro-cid-5grsw2hi><em data-astro-cid-5grsw2hi></em></p> <div class="relative pt-1" data-astro-cid-5grsw2hi> <img src="https://live.staticflickr.com/4256/35608224685_e639ee9960_h.jpg" alt="Mona Vale, Australia" data-astro-cid-5grsw2hi="true" width="1600" height="1067" loading="lazy" decoding="async" class="w-full"> <div class="absolute text-white top-1/3 left-1/3 transform -translate-x-1/4 -translate-y-1/4" data-astro-cid-5grsw2hi> <p class="text-2xl sm:text-3xl md:text-5xl font-extrabold drop-shadow-[0_4px_3px_rgb(0,0,0,0.5)]" data-astro-cid-5grsw2hi>Run Phi-3 SLM on your machine with C# Semantic Kernel and Ollama</p> <p class="py-2 text-sm sm:text-base" data-astro-cid-5grsw2hi>May 1, 2024</p> </div> <p class="absolute text-sm sm:text-xs text-white bottom-5 right-5" data-astro-cid-5grsw2hi>Mona Vale, Australia</p> </div> <div class="py-4" data-astro-cid-5grsw2hi>  <article class="prose prose-lg max-w-none dark:prose-invert
                    prose-h1:font-bold prose-h1:text-4xl prose-h1:mb-4
                    prose-a:text-blue-600 prose-a:no-underline
                    prose-p:text-justify prose-img:rounded-xl"> <p>Microsoft recently unveiled Phi-3, the latest iteration of their Small Language Model (SLM). And hot on its heels is Ollama, a powerful tool that enables you to run SLMs and LLMs right on your own machine.</p>
<p>Excited to dive in? In this guide, I’ll show you how to harness the power of Phi-3 and Ollama using C# and Semantic Kernel. I’ll walk you through the process of creating a simple console application to get you started on your SLM journey.</p>
<p>So, let’s get coding and unlock the potential of Phi-3 and Ollama on your machine!</p>

<h1 id="introduction">Introduction</h1>
<p>Phi-3 is the third generation of Microsoft’s Small Language Model (SLM). It’s a powerful tool that enables you to generate text based on a given prompt. Phi-3 is trained on a diverse range of data sources, making it capable of generating high-quality text across a wide variety of topics.</p>
<p>Ollama is a tool that allows you to run SLMs and LLMs on your own machine. It provides a simple and efficient way to interact with these models, enabling you to generate text quickly and easily. Ollama has built-in compatibility with the OpenAI Chat Completions API, making it easy to integrate them into your own applications.</p>
<p>In this guide, we’ll show you how to use Phi-3 and Ollama with C# and Semantic Kernel. We’ll walk you through the process of creating a simple console application that interacts with Phi-3 using Ollama. By the end of this guide, you’ll have a basic understanding of how to harness the power of Phi-3 and Ollama in your own applications.</p>
<h1 id="prerequisites">Prerequisites</h1>
<p>To get started, you’ll need to have to install Ollama on your machine and to download the Phi-3 model.</p>
<p>To install Ollama, you can download the installer <a href="https://www.ollama.com/download">from their download page</a> or just run <code>winget install ollama</code>.</p>
<p>Then you need to download the Phi-3 model using <code>ollama pull phi3:latest</code> which is around 2Gb.</p>
<h1 id="creating-a-simple-console-application">Creating a simple console application</h1>
<p>Now that you have Ollama and the Phi-3 model installed, let’s create a simple console application that interacts with Phi-3. We’ll use C# and Semantic Kernel to achieve this.</p>
<p>First, create a new console application and navigate to the project directory using</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8;overflow-x:auto" tabindex="0" data-language="powershell"><code><span class="line"><span style="color:#E1E4E8">dotnet new console </span><span style="color:#F97583">-</span><span style="color:#E1E4E8">n Phi3SKConsoleApp</span></span>
<span class="line"><span style="color:#E1E4E8">cd Phi3SKConsoleApp</span></span></code></pre>
<p>Next, add the Semantic Kernel package to your project using .</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8;overflow-x:auto" tabindex="0" data-language="powershell"><code><span class="line"><span style="color:#E1E4E8">dotnet add package Microsoft.SemanticKernel</span></span></code></pre>
<p>Now, open the <code>Program.cs</code> file in your favorite code editor and add the following code:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8;overflow-x:auto" tabindex="0" data-language="csharp"><code><span class="line"><span style="color:#F97583">using</span><span style="color:#B392F0"> Microsoft</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">SemanticKernel</span><span style="color:#E1E4E8">;</span></span>
<span class="line"><span style="color:#F97583">using</span><span style="color:#B392F0"> System</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Text</span><span style="color:#E1E4E8">;</span></span>
<span class="line"><span style="color:#F97583">using</span><span style="color:#B392F0"> Microsoft</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">SemanticKernel</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">ChatCompletion</span><span style="color:#E1E4E8">;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">// Initialize the Semantic kernel</span></span>
<span class="line"><span style="color:#F97583">var</span><span style="color:#B392F0"> kernelBuilder</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> Kernel.</span><span style="color:#B392F0">CreateBuilder</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#F97583">var</span><span style="color:#B392F0"> kernel</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> kernelBuilder</span></span>
<span class="line"><span style="color:#E1E4E8">    .</span><span style="color:#B392F0">AddOpenAIChatCompletion</span><span style="color:#E1E4E8">(                        </span><span style="color:#6A737D">// We use Semantic Kernel OpenAI API</span></span>
<span class="line"><span style="color:#B392F0">        modelId</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">&quot;phi3&quot;</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#B392F0">        apiKey</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">null</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#B392F0">        endpoint</span><span style="color:#E1E4E8">: </span><span style="color:#F97583">new</span><span style="color:#B392F0"> Uri</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">&quot;http://localhost:11434&quot;</span><span style="color:#E1E4E8">)) </span><span style="color:#6A737D">// With Ollama OpenAI API endpoint</span></span>
<span class="line"><span style="color:#E1E4E8">    .</span><span style="color:#B392F0">Build</span><span style="color:#E1E4E8">();</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">// Create a new chat</span></span>
<span class="line"><span style="color:#F97583">var</span><span style="color:#B392F0"> ai</span><span style="color:#F97583"> =</span><span style="color:#E1E4E8"> kernel.</span><span style="color:#B392F0">GetRequiredService</span><span style="color:#E1E4E8">&lt;</span><span style="color:#B392F0">IChatCompletionService</span><span style="color:#E1E4E8">&gt;();</span></span>
<span class="line"><span style="color:#B392F0">ChatHistory</span><span style="color:#B392F0"> chat</span><span style="color:#F97583"> =</span><span style="color:#F97583"> new</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">&quot;You are an AI assistant that helps people find information.&quot;</span><span style="color:#E1E4E8">);</span></span>
<span class="line"><span style="color:#B392F0">StringBuilder</span><span style="color:#B392F0"> builder</span><span style="color:#F97583"> =</span><span style="color:#F97583"> new</span><span style="color:#E1E4E8">();</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">// User question &amp; answer loop</span></span>
<span class="line"><span style="color:#F97583">while</span><span style="color:#E1E4E8"> (</span><span style="color:#79B8FF">true</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">{</span></span>
<span class="line"><span style="color:#E1E4E8">    Console.</span><span style="color:#B392F0">Write</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">&quot;Question: &quot;</span><span style="color:#E1E4E8">);</span></span>
<span class="line"><span style="color:#E1E4E8">    chat.</span><span style="color:#B392F0">AddUserMessage</span><span style="color:#E1E4E8">(Console.</span><span style="color:#B392F0">ReadLine</span><span style="color:#E1E4E8">()</span><span style="color:#F97583">!</span><span style="color:#E1E4E8">);</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">    builder.</span><span style="color:#B392F0">Clear</span><span style="color:#E1E4E8">();</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">    // Get the AI response streamed back to the console</span></span>
<span class="line"><span style="color:#F97583">    await</span><span style="color:#F97583"> foreach</span><span style="color:#E1E4E8"> (</span><span style="color:#F97583">var</span><span style="color:#B392F0"> message</span><span style="color:#F97583"> in</span><span style="color:#E1E4E8"> ai.</span><span style="color:#B392F0">GetStreamingChatMessageContentsAsync</span><span style="color:#E1E4E8">(chat, </span><span style="color:#B392F0">kernel</span><span style="color:#E1E4E8">: kernel))</span></span>
<span class="line"><span style="color:#E1E4E8">    {</span></span>
<span class="line"><span style="color:#E1E4E8">        Console.</span><span style="color:#B392F0">Write</span><span style="color:#E1E4E8">(message);</span></span>
<span class="line"><span style="color:#E1E4E8">        builder.</span><span style="color:#B392F0">Append</span><span style="color:#E1E4E8">(message.Content);</span></span>
<span class="line"><span style="color:#E1E4E8">    }</span></span>
<span class="line"><span style="color:#E1E4E8">    Console.</span><span style="color:#B392F0">WriteLine</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#E1E4E8">    chat.</span><span style="color:#B392F0">AddAssistantMessage</span><span style="color:#E1E4E8">(builder.</span><span style="color:#B392F0">ToString</span><span style="color:#E1E4E8">());</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">    Console.</span><span style="color:#B392F0">WriteLine</span><span style="color:#E1E4E8">();</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span></code></pre>
<p>We use the <code>Microsoft.SemanticKernel</code> package to interact with the Phi-3 model through Ollama. We create a new chat and loop through a series of user questions and AI responses. The AI response is streamed back to the console, allowing you to interact with the Phi-3 model in real-time. And this, in 37 lines of code!</p>
<p>The trick is to use the <code>OpenAIChatCompletion</code> service from the Semantic Kernel and to provide the Ollama OpenAI API endpoint at <code>http://localhost:11434</code> 🤯 specifying the model we downloaded <code>phi3</code>.</p>
<h1 id="running-the-console-application">Running the console application</h1>
<p>Make sure that Ollama is running on your machine before running the console application.</p>
<p>To run the console application, simply <code>dotnet run</code>.</p>
<p><img src="/images/Phi-3-Semantic-Kernel-Ollama.gif" alt="Run Phi-3 SLM on your machine with C# Semantic Kernel and Ollama"/></p>
<h1 id="conclusion">Conclusion</h1>
<p>In this blog post, I’ve shown you how to harness the power of Phi-3 and Ollama using C# and Semantic Kernel. We’ve created a simple console application that interacts with the Phi-3 model, allowing you to generate text based on a given prompt. This is just the tip of the iceberg when it comes to the capabilities of Semantic Kernel, Phi-3 and Ollama, so I encourage you to explore further and see what you can create!</p>
<h1 id="references">References</h1>
<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/">Introducing Phi-3: Redefining what’s possible with SLMs</a></li>
<li><a href="https://www.ollama.com/">Ollama</a></li>
<li><a href="https://github.com/ollama/ollama/blob/main/docs/openai.md">Ollama OpenAI compatibility</a></li>
<li><a href="https://www.ollama.com/library">Ollama Models</a></li>
<li><a href="https://learn.microsoft.com/en-us/semantic-kernel/overview/?tabs=Csharp">What is Semantic Kernel?</a></li>
</ul>
<p>Get the source code on GitHub <a href="https://github.com/laurentkempe/aiPlayground/tree/main/Phi3SKConsoleApp">laurentkempe/aiPlayground/Phi3SKConsoleApp</a></p>
<div class="my-5"> <a class="block max-w-[700px] p-8 rounded-lg border border-text/5 bg-text/[0.025] transition-all duration-200 ease-in-out hover:bg-text/[0.12] hover:border-text/10 text-text no-underline" href="https://github.com/laurentkempe/aiPlayground" target="_blank" style="color: inherit; text-decoration: none;"> <img class="w-20 h-20 rounded-full float-right ml-8 mb-8 mt-2.5" src="https://avatars.githubusercontent.com/u/272612?v=4" alt="laurentkempe avatar"> <p class="text-[1.8rem] tracking-wide m-0"> <span>laurentkempe/</span><b>aiPlayground</b> </p> <p class="text-[0.9rem] my-2.5"> Some experiments around AI to learn. </p> <div class="flex items-center gap-16"> <div class="flex flex-row gap-2.5 justify-center"> <svg class="w-[1.1rem] h-[1.1rem] mt-1.5 text-text/70" viewBox="0 0 16 16" fill="currentColor"> <path d="M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z"></path> </svg> <div class="flex flex-col"> <b class="text-[1.2rem]">11</b> <span class="text-[0.8rem] opacity-70">Stars</span> </div> </div> <div class="flex flex-row gap-2.5 justify-center"> <svg class="w-[1.1rem] h-[1.1rem] mt-1.5 text-text/70" viewBox="0 0 16 16" fill="currentColor"> <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z"></path> </svg> <div class="flex flex-col"> <b class="text-[1.2rem]">4</b> <span class="text-[0.8rem] opacity-70">Forks</span> </div> </div> <div class="flex flex-row gap-2.5 justify-center"> <svg class="w-[1.1rem] h-[1.1rem] mt-1.5 text-text/70" viewBox="0 0 16 16" fill="currentColor"> <path d="M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path> <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z"></path> </svg> <div class="flex flex-col"> <b class="text-[1.2rem]">0</b> <span class="text-[0.8rem] opacity-70">Issues</span> </div> </div> </div> </a> </div> </article>  </div> <div class="tags" data-astro-cid-5grsw2hi> <p class="tag" data-astro-cid-5grsw2hi><a href="/tags/Semantic Kernel" data-astro-cid-5grsw2hi>Semantic Kernel</a></p><p class="tag" data-astro-cid-5grsw2hi><a href="/tags/C#" data-astro-cid-5grsw2hi>C#</a></p><p class="tag" data-astro-cid-5grsw2hi><a href="/tags/Ollama" data-astro-cid-5grsw2hi>Ollama</a></p><p class="tag" data-astro-cid-5grsw2hi><a href="/tags/LLM" data-astro-cid-5grsw2hi>LLM</a></p><p class="tag" data-astro-cid-5grsw2hi><a href="/tags/SLM" data-astro-cid-5grsw2hi>SLM</a></p><p class="tag" data-astro-cid-5grsw2hi><a href="/tags/AI" data-astro-cid-5grsw2hi>AI</a></p> </div>  </div> <footer class="py-8"> <div class="container mx-auto gap-4 px-4 text-sm text-gray-600 text-center"> <div>
&copy; 2025 Laurent Kempé. All rights reserved.
</div> <div>
The expressed opinions are my own and do not reflect those of my employer or any third-party entities.
</div> </div> </footer> <script src="https://cdn.vidstack.io/player.core" type="module"></script> <script src="https://cdn.vidstack.io/icons" type="module"></script> </body> </html> 