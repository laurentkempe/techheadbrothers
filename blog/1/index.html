<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/ico" href="/favicon.ico"><link rel="stylesheet" href="https://cdn.vidstack.io/player.css"><meta name="generator" content="Astro v5.2.5"><title>Laurent Kempé - Blog</title><link rel="stylesheet" href="/_astro/_slug_.DKcIe2eh.css"></head> <body> <header class="py-6 border-b container mx-auto px-4 sm:px-6 lg:px-8 max-w-7xl"> <nav class="container mx-auto px-4 flex flex-col sm:flex-row items-center justify-between"> <div class="text-2xl font-bold mb-4 sm:mb-0"> <a href="/">Laurent Kempé</a> </div> <div class="flex gap-6 text-gray-600"> <a href="/" class="hover:text-gray-900 ">Home</a> <a href="/blog/1" class="hover:text-gray-900 text-blue-600">Blog</a> <a href="/speaking" class="hover:text-gray-900 ">Speaking</a> <a href="/about-laurent-kempe" class="hover:text-gray-900 ">About</a> </div> </nav> </header> <div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-7xl"> <article class="bg-white p-6 flex flex-col md:flex-row"> <div class="flex-1"> <h2 class="text-2xl font-bold"><a href="/2025/02/01/harnessing-deepseek-r1-with-dotnet-aspire-and-ollama-locally/">Harnessing DeepSeek-R1 distilled model with .NET Aspire and Ollama locally</a></h2> <p class="text-sm text-gray-500 mt-0">Feb 1, 2025</p> <p class="mb-4"><p>In my earlier posts, I've demonstrated how to install Ollama using the Windows installer. However, for developers, there's a more streamlined method to set up Ollama on your machine.</p>
<p>In this blog post, we'll explore how to run DeepSeek-R1 by harnessing the capabilities of .NET Aspire alongside Ollama on your local environment.</p>
</p> <p class="mt-2"><a href="/2025/02/01/harnessing-deepseek-r1-with-dotnet-aspire-and-ollama-locally/" class="text-blue-500 hover:underline">Continue reading</a></p> </div> <div class="ml-6 hidden md:block"> <a href="/2025/02/01/harnessing-deepseek-r1-with-dotnet-aspire-and-ollama-locally/"> <img src="https://live.staticflickr.com/65535/54288775875_cf354c8df3_q.jpg" alt="Liebvillers, France" width="150" height="150" loading="lazy" decoding="async" class="rounded-lg shadow-md"> </a> </div> </article><article class="bg-white p-6 flex flex-col md:flex-row"> <div class="flex-1"> <h2 class="text-2xl font-bold"><a href="/2025/01/27/leveraging-microsoftextensionsai-for-tool-calling-in-csharp/">Leveraging Microsoft.Extensions.AI for Tool Calling in C#</a></h2> <p class="text-sm text-gray-500 mt-0">Jan 27, 2025</p> <p class="mb-4"><p>In the previous post &quot;<a href="https://laurentkempe.com/2024/10/28/learning-ai-function-calling-in-csharp-with-llama-32-slm-and-ollama-running-on-your-machine/">Learning AI function calling in C# with Llama 3.2 SLM and Ollama running on your machine</a>&quot;, we wrapped our head around the concept of tool calling and implemented a C# source generator enabling our functions to be called by Llama 3.2 SLM using Ollama.</p>
<p>In this post, we will explore how to use Microsoft Extensions AI for tool calling in a simple .NET CLI application. We will leverage the power of Ollama and Llama 3.2 SLM to call functions and interact with the AI model using C#.</p>
</p> <p class="mt-2"><a href="/2025/01/27/leveraging-microsoftextensionsai-for-tool-calling-in-csharp/" class="text-blue-500 hover:underline">Continue reading</a></p> </div> <div class="ml-6 hidden md:block"> <a href="/2025/01/27/leveraging-microsoftextensionsai-for-tool-calling-in-csharp/"> <img src="https://live.staticflickr.com/65535/54288342071_75a4049e30_q.jpg" alt="Liebvillers, France" width="150" height="150" loading="lazy" decoding="async" class="rounded-lg shadow-md"> </a> </div> </article><article class="bg-white p-6 flex flex-col md:flex-row"> <div class="flex-1"> <h2 class="text-2xl font-bold"><a href="/2024/10/28/learning-ai-function-calling-in-csharp-with-llama-32-slm-and-ollama-running-on-your-machine/">Learning AI function calling in C# with Llama 3.2 SLM and Ollama running on your machine</a></h2> <p class="text-sm text-gray-500 mt-0">Oct 28, 2024</p> <p class="mb-4"><p>I've been trying to wrap my head around function/tool calling for a while now, and I'm excited to share what I've learned with you. It's a powerful way to let developers integrate advanced AI features directly into their applications. We'll walk through understanding the core concepts, setting up your environment, and implementing a practical example using a C# source generator.</p>
</p> <p class="mt-2"><a href="/2024/10/28/learning-ai-function-calling-in-csharp-with-llama-32-slm-and-ollama-running-on-your-machine/" class="text-blue-500 hover:underline">Continue reading</a></p> </div> <div class="ml-6 hidden md:block"> <a href="/2024/10/28/learning-ai-function-calling-in-csharp-with-llama-32-slm-and-ollama-running-on-your-machine/"> <img src="https://live.staticflickr.com/4303/36283818966_e3fb80e9eb_q.jpg" alt="Mona Vale, New South Wales, Australia" width="150" height="150" loading="lazy" decoding="async" class="rounded-lg shadow-md"> </a> </div> </article><article class="bg-white p-6 flex flex-col md:flex-row"> <div class="flex-1"> <h2 class="text-2xl font-bold"><a href="/2024/05/01/run-phi-3-slm-on-your-machine-with-csharp-semantic-kernel-and-ollama/">Run Phi-3 SLM on your machine with C# Semantic Kernel and Ollama</a></h2> <p class="text-sm text-gray-500 mt-0">May 1, 2024</p> <p class="mb-4"><p>Microsoft recently unveiled Phi-3, the latest iteration of their Small Language Model (SLM). And hot on its heels is Ollama, a powerful tool that enables you to run SLMs and LLMs right on your own machine.</p>
<p>Excited to dive in? In this guide, I'll show you how to harness the power of Phi-3 and Ollama using C# and Semantic Kernel. I'll walk you through the process of creating a simple console application to get you started on your SLM journey.</p>
<p>So, let's get coding and unlock the potential of Phi-3 and Ollama on your machine!</p>
</p> <p class="mt-2"><a href="/2024/05/01/run-phi-3-slm-on-your-machine-with-csharp-semantic-kernel-and-ollama/" class="text-blue-500 hover:underline">Continue reading</a></p> </div> <div class="ml-6 hidden md:block"> <a href="/2024/05/01/run-phi-3-slm-on-your-machine-with-csharp-semantic-kernel-and-ollama/"> <img src="https://live.staticflickr.com/4256/35608224685_3186752dc8_q.jpg" alt="Mona Vale, Australia" width="150" height="150" loading="lazy" decoding="async" class="rounded-lg shadow-md"> </a> </div> </article><div class="flex justify-between items-center p-6"> <div class="space-x-2">   <a href="/blog/2" class="text-blue-500 hover:underline">Next</a> <a href="/blog/6" class="text-blue-500 hover:underline">Last</a> </div> <div class="text-gray-600">
Page 1 of 6 </div> </div>  </div> <footer class="py-8"> <div class="container mx-auto gap-4 px-4 text-sm text-gray-600 text-center"> <div>
&copy; 2025 Laurent Kempé. All rights reserved.
</div> <div>
The expressed opinions are my own and do not reflect those of my employer or any third-party entities.
</div> </div> </footer> <script src="https://cdn.vidstack.io/player.core" type="module"></script> <script src="https://cdn.vidstack.io/icons" type="module"></script> </body> </html>